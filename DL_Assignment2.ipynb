{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jhiXe8y-QQJ2"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Complete the functions and the other sections below to build a complete model of a simple neural network (equivalent to Logistic Regression). \n",
        "The incomplete parts are marked and you need to fill them up to finish this assignment\n",
        "\n",
        "DATA: handwritten digits classified into two classes Even (0) and Odd (1)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "from sklearn import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "id": "UiPohyOJQQJ8",
        "outputId": "d166759c-c915-4728-e4d6-715ae706a2d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1597, 8, 8) (1, 1597) (200, 8, 8) (1, 200)\n",
            "y = [1], it's a 'odd' number.\n",
            "Number of training examples: m_train = 1597\n",
            "Number of testing examples: m_test = 200\n",
            "Height/Width of each image: num_px = 8\n",
            "Each image is of size: (8, 8)\n",
            "train_set_x shape: (1597, 8, 8)\n",
            "train_set_y shape: (1, 1597)\n",
            "test_set_x shape: (200, 8, 8)\n",
            "test_set_y shape: (1, 200)\n",
            "train_set_x_flatten shape: (64, 1597)\n",
            "train_set_y shape: (1, 1597)\n",
            "test_set_x_flatten shape: (64, 200)\n",
            "test_set_y shape: (1, 200)\n",
            "Cost after iteration 0: 0.693147\n",
            "Cost after iteration 100: 0.692835\n",
            "Cost after iteration 200: 0.692527\n",
            "Cost after iteration 300: 0.692221\n",
            "Cost after iteration 400: 0.691917\n",
            "Cost after iteration 500: 0.691613\n",
            "Cost after iteration 600: 0.691311\n",
            "Cost after iteration 700: 0.691009\n",
            "Cost after iteration 800: 0.690708\n",
            "Cost after iteration 900: 0.690407\n",
            "Cost after iteration 1000: 0.690107\n",
            "Cost after iteration 1100: 0.689807\n",
            "Cost after iteration 1200: 0.689507\n",
            "Cost after iteration 1300: 0.689207\n",
            "Cost after iteration 1400: 0.688908\n",
            "Cost after iteration 1500: 0.688610\n",
            "Cost after iteration 1600: 0.688311\n",
            "Cost after iteration 1700: 0.688013\n",
            "Cost after iteration 1800: 0.687716\n",
            "Cost after iteration 1900: 0.687418\n",
            "train accuracy: 74.6399499060739 %\n",
            "test accuracy: 78.5 %\n",
            "y = 1, you predicted that it is a \"odd\" number.\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Complete the functions and the other sections below to build a complete model of a simple neural network (equivalent to Logistic Regression). \n",
        "The incomplete parts are marked and you need to fill them up to finish this assignment\n",
        "\n",
        "DATA: handwritten digits classified into two classes Even (0) and Odd (1)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "from sklearn import datasets\n",
        "\n",
        "\n",
        "##Loading the data\n",
        "digits = datasets.load_digits()\n",
        "images_and_labels = list(zip(digits.images, digits.target))\n",
        "\n",
        "train_set_x_orig = [] \n",
        "train_set_y = []\n",
        "test_set_x_orig = [] \n",
        "test_set_y = []\n",
        "classes = ['even','odd']\n",
        "\n",
        "\n",
        "data_size = len(images_and_labels)\n",
        "#Setting the testset size\n",
        "test_set_size = 200\n",
        "\n",
        "\n",
        "##Splitting the data into training and test sets and assigning the labels: Even (0), Odd (1)\n",
        "\n",
        "for i in range(len(images_and_labels))[:-test_set_size]:\n",
        "    train_set_x_orig.append(images_and_labels[i][0])\n",
        "    if images_and_labels[i][1] %2 == 0: ##if even put 0 as the label\n",
        "        train_set_y.append(0)\n",
        "    else: ##if odd put 1 as the label\n",
        "        train_set_y.append(1)\n",
        "        \n",
        "for i in range(len(images_and_labels))[-test_set_size:]:\n",
        "    test_set_x_orig.append(images_and_labels[i][0])\n",
        "    if images_and_labels[i][1] %2 == 0: ##if even put 0 as the label\n",
        "        test_set_y.append(0)\n",
        "    else: ##if odd put 1 as the label\n",
        "        test_set_y.append(1)\n",
        "\n",
        "train_set_x_orig = np.array(train_set_x_orig)\n",
        "train_set_y = np.array(train_set_y).reshape(1,data_size-test_set_size)\n",
        "test_set_x_orig = np.array(test_set_x_orig)\n",
        "test_set_y = np.array(test_set_y).reshape(1,test_set_size)\n",
        "\n",
        "print(train_set_x_orig.shape, train_set_y.shape, test_set_x_orig.shape, test_set_y.shape)\n",
        "\n",
        "## Display an example from the data\n",
        "index = 1\n",
        "plt.imshow(train_set_x_orig[index])\n",
        "print (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])] +  \"' number.\")\n",
        "\n",
        "m_train = train_set_y.shape[1]\n",
        "m_test = test_set_y.shape[1]\n",
        "num_px = train_set_x_orig.shape[2]\n",
        "\n",
        "##Description of the data\n",
        "print (\"Number of training examples: m_train = \" + str(m_train))\n",
        "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
        "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
        "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \")\")\n",
        "print (\"train_set_x shape: \" + str(train_set_x_orig.shape))\n",
        "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
        "print (\"test_set_x shape: \" + str(test_set_x_orig.shape))\n",
        "print (\"test_set_y shape: \" + str(test_set_y.shape))\n",
        "\n",
        "# Reshape the training and test examples from 2D matrix to a 1D vector. The input vector of a neural network is always 1D\n",
        "train_set_x_flatten = train_set_x_orig.reshape(m_train,(num_px*num_px)).T\n",
        "test_set_x_flatten = test_set_x_orig.reshape(m_test,(num_px*num_px)).T\n",
        "\n",
        "# Print the description after reshaping\n",
        "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
        "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
        "print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
        "print (\"test_set_y shape: \" + str(test_set_y.shape))\n",
        "\n",
        "# Standardize the pixel value\n",
        "train_set_x = train_set_x_flatten/255.\n",
        "test_set_x = test_set_x_flatten/255.\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of z\n",
        "\n",
        "    Arguments:\n",
        "    z -- A scalar or numpy array of any size.\n",
        "\n",
        "    Return:\n",
        "    s -- sigmoid(z)\n",
        "    \"\"\"\n",
        "    s = 1/(1 + np.exp(-z))\n",
        "    \n",
        "    return s\n",
        "\n",
        "def initialize_with_zeros(dim):\n",
        "    \"\"\"\n",
        "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
        "    \n",
        "    Argument:\n",
        "    dim -- size of the w vector we want (or number of parameters in this case)\n",
        "    \n",
        "    Returns:\n",
        "    w -- initialized vector of shape (dim, 1)\n",
        "    b -- initialized scalar (corresponds to the bias)\n",
        "    \"\"\"\n",
        "\n",
        "    w = np.zeros([dim, 1])\n",
        "    b = 0.0\n",
        "    \n",
        "    ## veryifying the shape of the w vector\n",
        "    assert(w.shape == (dim, 1))\n",
        "    assert(isinstance(b, float) or isinstance(b, int))\n",
        "    \n",
        "    return w, b\n",
        "\n",
        "def propagate(w, b, X, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function and its gradient for the propagation explained above\n",
        "\n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of size (num_px * num_px, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if even and 1 if odd) of size (1, number of examples)\n",
        "\n",
        "    Return:\n",
        "    cost -- negative log-likelihood cost for logistic regression\n",
        "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
        "    db -- gradient of the loss with respect to b, thus same shape as b\n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    m = float(X.shape[1])\n",
        "    \n",
        "    # FORWARD PROPAGATION (FROM X TO COST)\n",
        "    A = sigmoid(np.dot(w.T,X) + b)\n",
        "    cost = -1/m * (np.dot(Y,np.log(A).T) + np.dot((1-Y),np.log(1 - A).T)) \n",
        "\n",
        "    # BACKWARD PROPAGATION (TO FIND GRADIENTS)\n",
        "    \n",
        "    dw = 1 / m *(np.dot(X,(A - Y).T))\n",
        "    db =  1 / m *(np.sum(A - Y))\n",
        "    \n",
        "    cost = np.squeeze(cost)\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return grads, cost\n",
        "\n",
        "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
        "    \"\"\"\n",
        "    This function optimizes w and b by running a gradient descent algorithm\n",
        "    \n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of shape (num_px * num_px, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if even, 1 if odd), of shape (1, number of examples)\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    print_cost -- True to print the loss every 100 steps\n",
        "    \n",
        "    Returns:\n",
        "    params -- dictionary containing the weights w and bias b\n",
        "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
        "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    costs = []\n",
        "    \n",
        "    for i in range(num_iterations):\n",
        "        \n",
        "                 \n",
        "        grads, cost = propagate(w,b,X,Y)\n",
        "\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "        \n",
        "        w = w - learning_rate*dw\n",
        "        b = b - learning_rate*db\n",
        "        \n",
        "        # Recordint the cost after every 100 iterations\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "        \n",
        "        # Print the cost every 100 training examples\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "    \n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return params, grads, costs\n",
        "\n",
        "def predict(w, b, X):\n",
        "    '''\n",
        "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
        "    \n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px, 1) in this case (8 * 8, 1)\n",
        "    b -- bias, a scalar \n",
        "    X -- data of size (num_px * num_px, number of examples) in this case (8 * 8, number of examples)\n",
        "    \n",
        "    Returns:\n",
        "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
        "    '''\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    Y_prediction = np.zeros((1,m))\n",
        "    w = w.reshape(X.shape[0], 1)\n",
        "\n",
        "    A = sigmoid(np.dot(w.T,X) + b)\n",
        "    \n",
        "    for i in range(A.shape[1]):\n",
        "        \n",
        "        # Convert probabilities A[0,i] to actual predictions p[0,i], i.e. if probability >=0.5 output 1, else 0\n",
        "        if(A[0][i] <= 0.5):\n",
        "            Y_prediction[0][i] = 0\n",
        "        else:\n",
        "            Y_prediction[0][i] = 1\n",
        "        \n",
        "    assert(Y_prediction.shape == (1, m))\n",
        "    \n",
        "    return Y_prediction\n",
        "\n",
        "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.005, print_cost = False):\n",
        "    \"\"\"\n",
        "    Builds the logistic regression model by calling the function you've implemented previously\n",
        "    \n",
        "    Arguments:\n",
        "    X_train -- training set represented by a numpy array of shape (num_px * num_px, m_train)\n",
        "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
        "    X_test -- test set represented by a numpy array of shape (num_px * num_px, m_test)\n",
        "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
        "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
        "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
        "    print_cost -- Set to true to print the cost every 100 iterations\n",
        "    \n",
        "    Returns:\n",
        "    d -- dictionary containing information about the model.\n",
        "    \"\"\"\n",
        "    \n",
        "    # initialize parameters with zeros \n",
        "    w, b = initialize_with_zeros(X_train.shape[0])\n",
        "\n",
        "    # Gradient descent \n",
        "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
        "\n",
        "    # Retrieve parameters w and b from dictionary \"parameters\" returned by the function above\n",
        "    w = parameters[\"w\"]\n",
        "    b = parameters[\"b\"]\n",
        "    \n",
        "    # Predict test/train set examples \n",
        "    Y_prediction_test = predict(w, b, X_test)\n",
        "    Y_prediction_train = predict(w, b, X_train)\n",
        "\n",
        "    # Print train/test Errors\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
        "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
        "\n",
        "    \n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": Y_prediction_test, \n",
        "         \"Y_prediction_train\" : Y_prediction_train, \n",
        "         \"w\" : w, \n",
        "         \"b\" : b,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "    \n",
        "    return d\n",
        "\n",
        "## Calling the function to train the model. FEEL FREE TO MODIFY THE HYPERPARAMETERS (num_iterations AND learning_rate) to experiment with the accuracy\n",
        "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.01, print_cost = True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Using the model you built to predict the digit of an image\n",
        "index = 24\n",
        "plt.imshow(test_set_x[:,index].reshape((num_px, num_px)))\n",
        "print (\"y = \" + str(test_set_y[0,index]) + \", you predicted that it is a \\\"\" + classes[int(d[\"Y_prediction_test\"][0,index])] +  \"\\\" number.\")\n",
        "\n",
        "## View how the cost varied with iterations\n",
        "costs = np.squeeze(d['costs'])\n",
        "plt.plot(costs)\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('iterations (per hundreds)')\n",
        "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpqr7ggK03EZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
